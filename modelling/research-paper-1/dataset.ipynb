{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fac70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting roboflow\n",
      "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.11.12)\n",
      "Collecting idna==3.7 (from roboflow)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
      "Collecting pi-heif<2 (from roboflow)\n",
      "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pillow-avif-plugin<2 (from roboflow)\n",
      "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
      "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
      "Collecting filetype (from roboflow)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.60.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
      "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.12.0.88\n",
      "    Uninstalling opencv-python-headless-4.12.0.88:\n",
      "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.11\n",
      "    Uninstalling idna-3.11:\n",
      "      Successfully uninstalled idna-3.11\n",
      "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"
     ]
    }
   ],
   "source": [
    "%pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a78f2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Attendance-System-Using-Face--2 to coco:: 100%|██████████| 32040/32040 [00:01<00:00, 26548.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Attendance-System-Using-Face--2 in coco:: 100%|██████████| 734/734 [00:00<00:00, 3227.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"1GlZktR4NmlZwEPbwo8V\")\n",
    "project = rf.workspace(\"gurukul\").project(\"attendance-system-using-face\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"coco\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accc0f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Attendance-System-Using-Face--2\n"
     ]
    }
   ],
   "source": [
    "print(dataset.location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05453710",
   "metadata": {},
   "source": [
    "### Checking Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab46ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root contents: ['test', 'train', 'valid', 'README.roboflow.txt', 'README.dataset.txt']\n",
      "Train contents: ['1-64-_jpg.rf.da8a456ab866c610c961d5cdf004701b.jpg', '1683268848381_1683268825008_jpg.rf.8097b82a75b92f5133e5cae092ed01c9.jpg', 'IMG20230505113443_jpg.rf.71ee97d964b94b7c6b919ca4b1cd4144.jpg', 'IMG20230505114756_jpg.rf.89f2450d2f6e805a33a7ee7cb137fb76.jpg', 'IMG_4751_JPG.rf.5fca8a32604d71cfaa01b752b2c384b4.jpg', 'IMG20230505114435_jpg.rf.35b564bc026cd1482bc74f3d6cec6676.jpg', 'IMG20230505114148_jpg.rf.df3039a131e4d3ca0db387cb21473bfd.jpg', '1683268849649_1683268825250_jpg.rf.ce6f57b8c7c3e02bd067fd03f893898f.jpg', 'IMG20230505114124_jpg.rf.028fde97a16152dc1f00c8cfd154e39d.jpg', 'IMG20230505114109_jpg.rf.0a5948697db93c06da37f32deee2fb83.jpg', '1-70-_jpg.rf.957550a9f071fe9ee02ef6fb9900ab2a.jpg', 'IMG20230505113538_jpg.rf.98ef4f91c06fae749a1ebdb6e723ae8a.jpg', 'IMG20230505114014_jpg.rf.552ac30ad4f9b47a263b00e520e74e3d.jpg', 'IMG20230505113321_jpg.rf.fd81317cf1a7d15c64f5d5959fd4a913.jpg', 'IMG20230505114522_jpg.rf.6aaeeaca24cbf1a0c0600ba9bacc2367.jpg', 'IMG20230505110717_jpg.rf.92cc409687a5a301cb0d32110325a22b.jpg', 'IMG20230505114312_jpg.rf.900ad39db408d0077cb179f4c005ec38.jpg', 'IMG20230505114031_jpg.rf.0f396b3b888530734feea121dfca82e3.jpg', '1-70-_jpg.rf.74d0b3db4a0e61bfd6bd00aad1dbb3d9.jpg', 'IMG20230505113311_jpg.rf.9550a0776275b2112c3966b6a67cbad0.jpg', '1-19-_jpg.rf.814a51ade7fb5ccf52f1e4a753b00d91.jpg', 'IMG20230505114458_jpg.rf.cd6a3593c37b1c368a6d146744a9fdc1.jpg', 'IMG20230505113957_jpg.rf.65e3e8e783839e16378dbb0a649ad4fe.jpg', '1-85-_jpg.rf.e179322b64ee0d5a52199bd087f37164.jpg', '1-89-_jpg.rf.b06961a3b15c88f40b9f4c79c80795dc.jpg', 'IMG20230505114448_jpg.rf.2c789dcef92e5f526a2d1ab2ad3b00af.jpg', 'IMG20230505110818_jpg.rf.eb23b0dd8394cd9e931301df0b917e77.jpg', '1-71-_jpg.rf.fe88fb383b78c3c91e90f5a283bf57ee.jpg', 'IMG20230505114341_jpg.rf.de7db1cbcf343c43b4166d989f023b00.jpg', '1-87-_jpg.rf.b25090bebbc939cd4c079f7a3517c8ff.jpg', 'IMG20230505114033_jpg.rf.c1ede1f43523bc0fbfbcb8e074689483.jpg', 'IMG20230505113311_jpg.rf.dbcf07977b95e5748c8c04bb0234e91b.jpg', '1-9-_jpg.rf.ea655da153d301554d9070b67e7b5659.jpg', 'IMG20230505114006_jpg.rf.93e8a4553a4bf34f15117c9b575d9c7e.jpg', 'IMG20230505114636_jpg.rf.ac1b47b238ee96900a29a2b89e9f3390.jpg', '1683268849053_1683268825145_jpg.rf.f91e4b217312734aea1c4ab8f6dbfc07.jpg', 'IMG20230505114002_jpg.rf.f60494ce4da4b8d71d3c223a18e7b05b.jpg', 'IMG20230505114312_jpg.rf.1a57d34bc20da1ea0377af7d5baecb8c.jpg', 'IMG20230505114551_jpg.rf.b096ac60bdaebd9c38097e58dc1686e7.jpg', '1-5-_jpg.rf.dd5264b73b27a2dd090da2357ec833b7.jpg', 'IMG20230505114455_jpg.rf.4c212261c8d36f75ffc03d6168ab93fa.jpg', 'IMG_4850_JPG.rf.500aececf77b7ddda935975478ea1714.jpg', 'IMG20230505114415_jpg.rf.daa66f561aba752d9d382fb758998acb.jpg', 'IMG20230505114346_jpg.rf.d46f01557067e7d06ddb0ea7ef730993.jpg', '1-60-_jpg.rf.e60892ef2cd9d97115d6e1cd03a78022.jpg', 'IMG_4779_JPG.rf.55d0088ca06b8c8b97d5c04530a77738.jpg', 'IMG_6838-2_jpg.rf.5d22bc98f47ffe7000cc74983ff167aa.jpg', 'IMG20230505114006_01_jpg.rf.37f79167ceb21e0f45e43af32990070f.jpg', '1-74-_jpg.rf.e132b491e33cf38cb1576179fb58164e.jpg', 'IMG20230505114206_jpg.rf.e8b7e8e0658a1b2ac4b0e16bb101cc01.jpg', '1-15-_jpg.rf.c56692854397cc59ef215fbf8cdd6403.jpg', 'IMG20230505114606_jpg.rf.2d0910c5a0cf38acd7962945d0f93dcb.jpg', '1-75-_jpg.rf.506e3aa7947d5b7fccf6c24cf5fb3a6a.jpg', 'IMG20230505114756_jpg.rf.43750d1fc620d72cc4df5a49a28a76f6.jpg', 'IMG20230505114340_jpg.rf.37e9d1b514823b4577a490bfa7c01dba.jpg', 'IMG20230505114148_jpg.rf.9a69667701447852735a9c782f58288e.jpg', 'IMG20230505113404_jpg.rf.c927bca17075843eb399d827dbacd8bc.jpg', '1-17-_jpg.rf.118d7671be6f79696df56f581cb4d4a6.jpg', 'IMG20230505113337_jpg.rf.8fdf5a1b6494f1d86d541174cd0b86d6.jpg', 'IMG20230505113339_jpg.rf.004e939c55c9da972aae1847d904316d.jpg', '1-64-_jpg.rf.d59bd52ff570f11a7ab5356c94c0b8c2.jpg', '1683268848421_1683268825016_jpg.rf.9a9443e5d8665976a2c964a74cc24140.jpg', 'IMG20230505114019_jpg.rf.dded0ef4648f3309989e3a9cf13c64e7.jpg', 'IMG20230505114132_jpg.rf.70505caab2dd4076a0b20f1ea9ff60c8.jpg', '1683268848876_1683268825112_jpg.rf.19fb684e562bcb8b4c46d6f9c8be04c8.jpg', '1-12-_jpg.rf.268175412e1368eb48c4fe858d474586.jpg', 'IMG20230505114209_jpg.rf.b1b017bff558cafe6a8618d7a1f6028c.jpg', '1-27-_jpg.rf.34b5b756fc5b09b2a229476536077977.jpg', 'IMG20230505114132_jpg.rf.d35d946310eccab9b17d0ccff4778f58.jpg', '1-89-_jpg.rf.1ba7c880c38382f34596b1770e17c628.jpg', 'IMG20230505114341_jpg.rf.6e7c3d13ec1a0b5f6e5480edda8c8a4c.jpg', '1-16-_jpg.rf.742c50bb53a754aee71e3893793c75dc.jpg', 'IMG_4869_JPG.rf.3faa5e3a8260a67b5fdce526d13dbace.jpg', 'IMG20230505114435_jpg.rf.5f7c5a153f8ee3c9d9ede60bcf54f718.jpg', 'IMG20230505114033_jpg.rf.ddbbf288afd054c4419237d909d11499.jpg', '1-21-_jpg.rf.43a51db67e2d330e494642fee4bc84b3.jpg', '1-14-_jpg.rf.66cffedc6b2cf644093d18ade34f01c1.jpg', 'IMG20230505114549_jpg.rf.d61cf529d34e68d58778f17365c69605.jpg', 'IMG_4766_JPG.rf.a5cf867b637b92e6809780b857ca617b.jpg', '1683268848280_1683268824992_jpg.rf.7ece1fda1f82c22e1c0f89a9d423b682.jpg', '1683268848131_1683268824947_jpg.rf.1181af90e477b24ec542552c42a4e1bd.jpg', 'IMG20230505114400_jpg.rf.857e2720980082b5494c95952a08b720.jpg', 'IMG_4956_JPG.rf.d57cac66a0cc3170134ef9211d2a1b84.jpg', 'IMG20230505114212_jpg.rf.fd3ab020ffae628544ec0cc703b083a7.jpg', 'IMG20230505114031_jpg.rf.fb2fd5f544b0a79aed47ab7f9d0fd9e2.jpg', '1-39-_jpg.rf.7a979c816ef2436caa33f63157c14290.jpg', 'IMG20230505113434_jpg.rf.f8b9a1f62706dc0007c08f3331d40d1b.jpg', 'IMG20230505110717_jpg.rf.a0e3e860f4a197ea823e4489c9a05c0a.jpg', 'IMG20230505114657_jpg.rf.311c23f8fe978c4f4c3c3436288693da.jpg', 'IMG20230505114205_jpg.rf.e29ad81e49de40be508a744da8ad49b9.jpg', '1-14-_jpg.rf.04378c837a426d2c5dc7183a86b62eee.jpg', 'IMG20230505113339_jpg.rf.f1a171de790c4a055a2d58733f98b6f2.jpg', 'IMG20230505114003_jpg.rf.ea8be01dbade5ff37909a189d38bd645.jpg', 'IMG20230505113456_jpg.rf.5d9be183056db987ee81a10a470ca5b3.jpg', '1-46-_jpg.rf.24bac44666db8c2872bd897e3258b944.jpg', '1-73-_jpg.rf.160608221e16e17dde80a2a36169963b.jpg', '1-63-_jpg.rf.455e70b7dcddb8a6af45df1241324557.jpg', '1-13-_jpg.rf.ae3ba9c891771f87df9955ad9b4613c3.jpg', 'IMG20230505113500_jpg.rf.f13b784488bc2e9740677982c21dcbdf.jpg', 'IMG20230505114636_jpg.rf.55ab42fd4574cadea720f763b36a2354.jpg', 'IMG20230505113258_jpg.rf.a1409e88562096878689db7f07aca7d9.jpg', '1683268848212_1683268824968_jpg.rf.738fd55664f575e54142d7a49fc9ff3e.jpg', '1683268849434_1683268825216_jpg.rf.bac131d47c4e851a6490e5d3b04691f5.jpg', '1683268848381_1683268825008_jpg.rf.143ffaf4a30370ea438dfd3f5ff6f862.jpg', '1683268848584_1683268825049_jpg.rf.01cd0dc3d5826e707a7de68631a17c2a.jpg', 'IMG20230505112450_jpg.rf.8a93bc63cd669e38da3d0222fd02dc14.jpg', '1683268848090_1683268824933_jpg.rf.cb7a1eecc0d293adb23ca77a7814abfe.jpg', '1-26-_jpg.rf.0ad00696bad69156cb7df3df189a1929.jpg', '1-15-_jpg.rf.74eed289a945a9dffdf3f831c7d9bf20.jpg', 'IMG20230505114348_01_jpg.rf.6096aefee59eab7a4633b573f9aa4072.jpg', 'IMG20230505114351_jpg.rf.867198de7215cbb8ea66ad1fd619c7a5.jpg', '1-40-_jpg.rf.0f0cead5c62e2db3c15f38e2b65d3c95.jpg', '1683268849714_1683268825263_jpg.rf.ff2b6009795e8f36917eca8e6ef83ee8.jpg', '1-9-_jpg.rf.91e2d1d7fac3a9141e2da1c8a6ad6a5c.jpg', 'IMG20230505114455_jpg.rf.3521816a15f2cdbed3f2871035fc37c9.jpg', '1683268849284_1683268825192_jpg.rf.799e8d6bdaf17f923e355119b737ec6a.jpg', '1-28-_jpg.rf.f083a39d5466d7fe939ca57e11051cfc.jpg', '1683268849489_1683268825226_jpg.rf.0b4f957d34702b40fde8599908592c39.jpg', 'IMG20230505114058_jpg.rf.ac6b97e282946e2485cbcbe90d83af42.jpg', 'IMG20230505114508_jpg.rf.a1717e90dd43c2d79d90cbc5a4a43225.jpg', '1683268849393_1683268825207_jpg.rf.7c1b2ee6e6b467de12b0eefd2f2dd117.jpg', '1-88-_jpg.rf.1c0f7e9cfc4a04e81255a8883cd05ba0.jpg', 'IMG20230505113500_jpg.rf.2246336a31d59e0e34efeeca51e2c215.jpg', '1-60-_jpg.rf.2ab6c02b6dbda7c2cdc67a385f004c01.jpg', 'IMG20230505114606_jpg.rf.b545d5098113457be598822e28cb3de6.jpg', 'IMG20230505114158_jpg.rf.7bc99939d7e4e4302869594aa4a62b06.jpg', 'IMG20230505114312_jpg.rf.81d24cd4ff57c6a10fa66db7f84e3047.jpg', '1-26-_jpg.rf.e19ae124a53ac7ba11f2cf55f9b87f20.jpg', 'IMG20230505113335_jpg.rf.0927f70540e8fb20e3ea0468a10a107f.jpg', '1-69-_jpg.rf.e5c54e573e939f347c0ede80a5fba960.jpg', 'IMG20230505110757_jpg.rf.8431f6f6b1d9ba5e329ced3a2ee8ef58.jpg', 'IMG20230505113508_jpg.rf.3e492af0d79fd14b13234c0456e2a4dd.jpg', '1-24-_jpg.rf.f9d7b5e32fc8004d8998b08900daedf7.jpg', 'IMG20230505113337_jpg.rf.3046181d0a349916871ec622df77aeb5.jpg', '1-85-_jpg.rf.f543ea520034ce94e144e41307ef89aa.jpg', '1-70-_jpg.rf.3306efc73e0208155f5183488d311263.jpg', '1-75-_jpg.rf.003c261858e4c9b4ea19d6db57440455.jpg', '1683268849587_1683268825233_jpg.rf.662b9b61636aedd4ceab988ca78e3c0f.jpg', 'IMG20230505113246_jpg.rf.6564da0c736aa08024bd9cf06491b3f8.jpg', '1683268849267_1683268825185_jpg.rf.85d259a33b87054a92672602b80bf28d.jpg', 'IMG20230505113307_jpg.rf.5dab6065e465efba00b7da3c55f61f2f.jpg', '1-13-_jpg.rf.a2efc08b412d04b908ffa5afef72bdaa.jpg', 'IMG20230505114721_jpg.rf.7a17c69bd4f4c3fde3d4ee10f1ec117b.jpg', 'IMG20230505114046_jpg.rf.39ea81f2daa8be3aa50fff2d3a4bd52a.jpg', 'IMG20230505114430_jpg.rf.d2ab9e04b251a7f21805a782ddcd7ee4.jpg', 'IMG20230505110737_jpg.rf.89b5274002f4d70bee1dfea0dae08d3c.jpg', 'IMG20230505113249_jpg.rf.059be592a9cf91967142b1c5e5f9d5b5.jpg', '1683268849008_1683268825136_jpg.rf.d3a5f83a700893ed4b91dd2bb772b4b9.jpg', '1683268849714_1683268825263_jpg.rf.9692e5006268824efcf0b892dfe29d47.jpg', '1-39-_jpg.rf.c17b43337baaef6c5a66ae4e410a8615.jpg', 'IMG20230505114220_jpg.rf.596e198c4ca43092fd0a89d1a57418f2.jpg', '1-8-_jpg.rf.e724fe3a236dcaf71cdeea7423c23ce0.jpg', 'IMG20230505110744_jpg.rf.eabe9bb824af8aebf88cd88d79be7061.jpg', '1-21-_jpg.rf.545343e73d453947ab08e9e23edf7a21.jpg', 'IMG20230505113258_jpg.rf.54e401d0976d108eab6b5544f3176a3d.jpg', '1-46-_jpg.rf.c7242d6fb3e0294dd4bdf17c2922a24f.jpg', 'IMG20230505114033_jpg.rf.b28813549955dcd0519c4641e7059fc2.jpg', '1-74-_jpg.rf.26b3a1e98615fdb392a84aaa390a215e.jpg', '1-16-_jpg.rf.a2e9c7c636cfb97adc6d3beef71b706b.jpg', '1-11-_jpg.rf.35b23d1d21a5d0cd5b1fceae18654f0d.jpg', 'IMG20230505113957_jpg.rf.e08d8765393cad818bacee2616a14ba1.jpg', 'IMG20230505114246_jpg.rf.d246be01c500ba393667dd2fa0d1bfa3.jpg', 'IMG20230505114430_jpg.rf.7927e675a422237478be128de2435337.jpg', '1683268848844_1683268825100_jpg.rf.718cbddecd10e076e3e303c6e30737ee.jpg', 'IMG20230505114046_jpg.rf.903c8fb1521ef55deb15f2cebd57cb24.jpg', 'IMG_4854_JPG.rf.d6223b3340109814cd871be98020f5e2.jpg', 'IMG_4950_JPG.rf.cfcec410b9568f134a9e1c496e85b80d.jpg', 'IMG20230505114105_jpg.rf.1cfb99ee8db32f34c0fdc95fe396682b.jpg', 'IMG20230505114348_jpg.rf.213673852afd7ccffb8cb1d7ea1de637.jpg', '1-55-_jpg.rf.09536f18295d53822b39558346e509a6.jpg', '1-3-_jpg.rf.2a85556fb60ec3cd8f00bab4e1ed581a.jpg', '1683268849393_1683268825207_jpg.rf.ee4c5f237596d7f928c58a6c628bf30f.jpg', '1-28-_jpg.rf.c1462259ab25cbe73e98b3d35edb94f8.jpg', 'IMG20230505114212_jpg.rf.69c7dd9b786954cd5d3582ddc424ae41.jpg', 'IMG20230505113321_jpg.rf.c36c3db29c0a3507f26786105865b31e.jpg', 'IMG20230505114259_jpg.rf.36675e4a93c50f188913b1c0f6b6a904.jpg', 'IMG_4956_JPG.rf.98c684076d9ee29b4c8e772ee38c2895.jpg', 'IMG20230505114616_jpg.rf.959543af6a806e9ac223958e5d32a205.jpg', '1-59-_jpg.rf.8365b98a6b4b44e323d1822e20d686b6.jpg', 'IMG20230505114028_jpg.rf.75f21ae314c9c942e6cdccdd85ca271a.jpg', 'IMG20230505114109_jpg.rf.7013730409b1056f78deee2635755cc0.jpg', '1-81-_jpg.rf.1c1880446fedcbfd6582c41bb0a45b23.jpg', 'IMG_4805_JPG.rf.cc34300d7cad6f00f3740ab703b4e84f.jpg', 'IMG20230505114348_01_jpg.rf.38bfe3e56c50ace277aad6b3cbcfbd82.jpg', 'IMG20230505113505_jpg.rf.810d5242037e46f4c470b3cab4a57412.jpg', 'IMG_4850_JPG.rf.2c1eda0833d639758636d50d4bcb86f4.jpg', 'IMG20230505114514_jpg.rf.de8c79a84a624b947b712f55761be311.jpg', '1-60-_jpg.rf.b85fb90a165414a52e550fd416308e65.jpg', 'IMG20230505113502_jpg.rf.3aff859648545f432606bd5306f5d0a5.jpg', '1-26-_jpg.rf.773351e35a137c171035033d6799cb69.jpg', 'IMG20230505114223_jpg.rf.14b943cf4b1bc5c8b6fdd1e8b5e61a20.jpg', '1-3-_jpg.rf.f42d44d97af0bc4a07eed2948cefa2c9.jpg', 'IMG20230505114758_jpg.rf.f8c4453070759955ed456349908ab019.jpg', 'IMG20230505114223_jpg.rf.10014fdbf07d86f7d005a4ad8eeca8c1.jpg', '1683268848656_1683268825057_jpg.rf.89f0708af6b86ff95f0e9c507a86892f.jpg', '1-37-_jpg.rf.8a4fbe4ca2d5f4ec9070b0510cd32a43.jpg', 'IMG20230505113403_jpg.rf.6678d7d76ed26eaf954433d06daddac9.jpg', 'IMG20230505114400_jpg.rf.4c882170441f09ab18e667fbb196ee66.jpg', 'IMG20230505113314_jpg.rf.3bad5b28374d436356f1d4abebbb4672.jpg', 'IMG20230505114259_jpg.rf.1bf725f539b8cabfed1425f48bb0d3ab.jpg', '1-84-_jpg.rf.ae96855352412f1f40d9295edd81c990.jpg', '1-82-_jpg.rf.08608a2f1b1e2e7a583444a2c30b6ed6.jpg', 'IMG20230505113307_jpg.rf.f3fd9ca53ca8bc78c81c585e71faa279.jpg', '1-87-_jpg.rf.9c39249661cf4718fd3edab174e7be1f.jpg', 'IMG20230505114455_jpg.rf.ffd3fbffeb5fbf47daa9cac0b0014924.jpg', 'IMG20230505114415_jpg.rf.4f9509a3b151953cc7df32a50427480a.jpg', 'IMG20230505114249_jpg.rf.c10b8889e23d29d2c351a600eefee31a.jpg', 'IMG20230505114551_jpg.rf.bc202c1a6e26c4349495f0562ca405f4.jpg', 'IMG20230505113404_jpg.rf.683ceebe0ef9d0bf5ee682d26f3b90bf.jpg', 'IMG20230505110757_jpg.rf.e69ef90b7cccd9c92d5d2062bec5cc3a.jpg', '1683268848876_1683268825112_jpg.rf.099715b7c035b4228967ee3b37cb8274.jpg', 'IMG20230505113456_jpg.rf.2e10f2f7131ffae94ac979fb49ff91c5.jpg', 'IMG20230505113347_jpg.rf.af40a633150bb645e733847a592f92ec.jpg', 'IMG_4766_JPG.rf.65d2bd11da970f83f69b87935464d319.jpg', '1-13-_jpg.rf.52d7bec6a924892290796bc16ac29f1f.jpg', 'IMG_4805_JPG.rf.391cb531f3521f860435277fd87a967b.jpg', 'IMG_4766_JPG.rf.e42119e1d5393bf739e7ba829adae19a.jpg', 'IMG20230505114506_jpg.rf.8d1387186b67a01096bbb3ddc180f6c3.jpg', '1683268849393_1683268825207_jpg.rf.6c0d42a011cdbcb6c52c3336617c0e5d.jpg', 'IMG20230505114508_jpg.rf.08313ffcc86bae2ad6698e14213326f2.jpg', '1-20-_jpg.rf.db1020d4810f10f236f541b5974a1942.jpg', '1-89-_jpg.rf.30243971952f5153186fae728d61e933.jpg', 'IMG20230505114249_jpg.rf.7b1269f73e5bd2d9c05e6e87c2587baf.jpg', 'IMG20230505113538_jpg.rf.c28e1a4bf6c7df081f55f0135ecd2199.jpg', '1-71-_jpg.rf.469537d3ce1528b84f4e526d659cbdc2.jpg', '1-56-_jpg.rf.78e2288360d80bc77b4dd24a9839a620.jpg', 'IMG20230505114739_jpg.rf.84bf5ac44780c897657cdd42df402afe.jpg', 'IMG20230505114038_jpg.rf.4ed4812af49e9fd6446dfc223dedd4e0.jpg', '1-81-_jpg.rf.6b310248742a5ba274e51562f865a292.jpg', 'IMG20230505114022_jpg.rf.85e6c29fb2852c9ca04af35c79be82f7.jpg', 'IMG20230505112658_jpg.rf.c0d146f403c205989da7204487f37ab3.jpg', 'IMG20230505114246_jpg.rf.7e4353c9e0d70283061aa1df489cc771.jpg', 'IMG_4961_JPG.rf.b9bf95e79197142603017fd1a28d7efc.jpg', '1683268847975_1683268824891_jpg.rf.0b19833ed2081f39a11d16748b3772dd.jpg', 'IMG20230505114144_jpg.rf.823de195867f80d6f3741ee367ecfcac.jpg', 'IMG_4850_JPG.rf.c8155b67478bd19a453f924c0c57a36d.jpg', 'IMG20230505114415_jpg.rf.fe02152779b2584225dbfcc579c1762f.jpg', '1-37-_jpg.rf.0bad5e8417e152f1269811970746952e.jpg', '1-29-_jpg.rf.25bb7cb38493545cc5ef7fd96475ce87.jpg', '1683268848876_1683268825112_jpg.rf.71d4494cc8c356e8624bf90aef3ce37a.jpg', '1-17-_jpg.rf.a3a49f8b60afb90890be5cd9b0cfd95a.jpg', 'IMG20230505114518_jpg.rf.f5ecd779283fe6f1154e74579275b34f.jpg', 'IMG_4805_JPG.rf.b108c18a3dd5207f0d580cb53679eb82.jpg', 'IMG20230505114340_jpg.rf.004b9f3b10cd09d12e3a28917c240d40.jpg', 'IMG20230505110737_jpg.rf.0209aa1bd2269deb1fbbadc476f50b3c.jpg', 'IMG20230505114148_jpg.rf.f6af94901cb43d26eb0ddb2ffd2929a2.jpg', 'IMG20230505113403_jpg.rf.49b6cca9691728ca6e11a4a05795d493.jpg', 'IMG20230505114130_jpg.rf.e69c4594b508c68f7066346c1fa81796.jpg', '1-32-_jpg.rf.c3bd001bcf904b10be6e7cff3b86375b.jpg', 'IMG20230505114657_jpg.rf.f8b60c90474e9cf23b9d33ef147dba54.jpg', '1-52-_jpg.rf.50b900e72de3551e8d4bdc6cc30038bf.jpg', 'IMG20230505114215_jpg.rf.c4c781d07fdb64d5b41fbf3c9236e108.jpg', '1-69-_jpg.rf.c2280f39f28444d7a857a5619fe981cd.jpg', '1683268849649_1683268825250_jpg.rf.1eef3f9ca88f62ff0381734f44c9a228.jpg', '1-62-_jpg.rf.2ecc7618c3a946b9b4585a18a49cd92d.jpg', '1-91-_jpg.rf.577950cbfbbf88bdf07abf68fc49d0ec.jpg', '1-55-_jpg.rf.e1b32a169f6485b8c497d4d6eea82b33.jpg', 'IMG20230505114435_jpg.rf.d63e08549a36747050984e97f7bca51f.jpg', 'IMG20230505114010_jpg.rf.a53bdca4f48c72480a70b463ab053200.jpg', 'IMG20230505114341_jpg.rf.90a62f62cbf8a72620441c1190ac3016.jpg', 'IMG20230505114554_jpg.rf.52a5ad3882f3a6c8338b2d283cacf852.jpg', '1683268848947_1683268825128_jpg.rf.d4ef0cc8a116dd6489ee78a3c706ad22.jpg', '1-63-_jpg.rf.a369b9a64b5208f75452a3e1f14873db.jpg', 'IMG20230505114006_jpg.rf.481ac21efcc34e8e1e207113a46dc28d.jpg', 'IMG20230505114348_jpg.rf.5cdd448b4206d465ebf64534e81c50f8.jpg', '1-42-_jpg.rf.2939f7a8a3c7c8aebf31a43fb0c0dd62.jpg', '1-4-_jpg.rf.40581ccb034984666159f8eaecb1875d.jpg', 'IMG20230505114006_01_jpg.rf.903eb2cb8c0d11f5e5b9c340b17cf7ca.jpg', '1-3-_jpg.rf.18a1abc544ae8e4f01cac193b9663e19.jpg', 'IMG20230505114758_jpg.rf.de1fe48d684f8e468d7dbe5505951088.jpg', 'IMG20230505113335_jpg.rf.d949f075f3fd7d6d9ed145616b435d3d.jpg', 'IMG20230505113301_jpg.rf.bd11d8996d1c7099802b8ce1c45d6fad.jpg', 'IMG20230505114042_jpg.rf.8cc47a0e29938495641565a578160c62.jpg', 'IMG20230505114143_jpg.rf.05e27d340542fa107387616f3444aa83.jpg', '1-40-_jpg.rf.8a047258cbd8a16b372ecc9515229e03.jpg', '1-73-_jpg.rf.9931c095d27de387c2ac59ef95955614.jpg', '1-11-_jpg.rf.9246afac0999502178042e8e422fe216.jpg', 'IMG20230505114144_jpg.rf.f724fa32c8d3a4143c0127464fcf3e0d.jpg', 'IMG20230505114229_jpg.rf.47193bb36c178516f405e30aa39bb950.jpg', '1-6-_jpg.rf.4037c3fc38b1a30fe87e4e705fe65125.jpg', '1683268849434_1683268825216_jpg.rf.e42e8a7d5e9f7d168cb042f18b150ebc.jpg', '1683268849489_1683268825226_jpg.rf.a81915b3f154921439f391c845a0f7bc.jpg', 'IMG20230505113500_jpg.rf.fe1d64595d1252a37c282f4dd1714494.jpg', 'IMG_6838-2_jpg.rf.90e97fdeeb8872ee907aeef90c0d40d0.jpg', '1683268848131_1683268824947_jpg.rf.fabb6c6a7aef85bc2118047df9b290a5.jpg', 'IMG20230505113240_jpg.rf.c26c992f0079401b14412ce0222ecb36.jpg', 'IMG20230505113314_jpg.rf.c85c6f2e4dba822321851f7fd13ada0b.jpg', 'IMG20230505114319_jpg.rf.20ca1cb0d58e53e14d6d040c3c986a2e.jpg', '1-85-_jpg.rf.18cea37aa125a8f2717c6fb6b6b0f6ae.jpg', '1-82-_jpg.rf.9eec522e4263f82f5c7f95d1b0f7cb8a.jpg', 'IMG20230505114143_jpg.rf.77a8e310ee6e4d0870c23810c9966dc1.jpg', 'IMG20230505114037_jpg.rf.7547c81aed3f652f8fb5d7f3257709a0.jpg', '1-58-_jpg.rf.d128bfce40168cc2f2156dd63900a63f.jpg', 'IMG20230505114105_jpg.rf.f8d96d0f5752394b5dfc9edb93dcb94e.jpg', '1-83-_jpg.rf.11b0ac7a1e4589e27a676f628a3dc16d.jpg', '1-59-_jpg.rf.aa17babfc093117ef013033ade9b3d5b.jpg', 'IMG20230505114417_jpg.rf.21ec8bb1cca8538d5a1550b2cfb9da6a.jpg', '1683268849617_1683268825242_jpg.rf.2ab84c93489beffa91455581bd50ae6d.jpg', 'IMG20230505113505_jpg.rf.b6b3d12c0e0f0bdb9291df96f08144c7.jpg', '1683268849008_1683268825136_jpg.rf.f1beca5508c3b9bf72cc9a94b7455aeb.jpg', 'IMG20230505114058_jpg.rf.c72b7e62a83324d943b15ca79bcb41a4.jpg', '1-22-_jpg.rf.1916a9ef2b13859d9cf19dc8815c1bd8.jpg', 'IMG20230505113403_jpg.rf.dc3fab18c46f782119b73f227fd96be2.jpg', 'IMG20230505110725_jpg.rf.07c41b859b0032250a9500d6e6bb60ed.jpg', '1683268848338_1683268825000_jpg.rf.9fe9f73435726ba4b55e59ec722a7937.jpg', 'IMG20230505113235_jpg.rf.ec77afeff70ce304f9fcaddd2241c327.jpg', 'IMG_4950_JPG.rf.14bda41cbeee8a8ea2573d0de7da51aa.jpg', '1-66-_jpg.rf.8db653a14adc14137975b3cb192256d1.jpg', 'IMG20230505114130_jpg.rf.3a3420cb2c2bad2ae1916ebb25a6a318.jpg', '1-56-_jpg.rf.b9c3fd358681f3797e949486481911b8.jpg', 'IMG20230505110710_jpg.rf.80126905961223be304d1bb3cdaa8d78.jpg', 'IMG20230505114616_jpg.rf.1526cabdbb3021a4cd0f4e2304c58bb1.jpg', '1683268849233_1683268825178_jpeg.rf.80900fb716b52064df1ca2b926083adc.jpg', 'IMG20230505112450_jpg.rf.02e4cc5b16bafa3b48cf051dda4422ca.jpg', '1-9-_jpg.rf.28d7c7e4f9d96f6ff29a627a78e66352.jpg', '1-69-_jpg.rf.181a1076e1d251d29540021cd99eb458.jpg', '1-16-_jpg.rf.77d6bb9423537323b12395796081d8a3.jpg', '1683268848947_1683268825128_jpg.rf.9fe56bcd63a3bbb5ca7c992a7ec4017d.jpg', 'IMG20230505114028_jpg.rf.7f3bf69ff49e842e1b4d4b41a8827339.jpg', 'IMG20230505114324_jpg.rf.d438a16deaba0aaa6699cb264c90b600.jpg', '1-52-_jpg.rf.f2076c51c97b6e19e1b0ef51f3c139be.jpg', '1-20-_jpg.rf.12678dfa2d00e132b70e2521ec07530b.jpg', '1-4-_jpg.rf.55c5c2816f10818033417967fa4370da.jpg', 'IMG20230505114150_jpg.rf.b2db10f67e79d47b8c2ab14417b8f0e2.jpg', '1-42-_jpg.rf.5ff0fca9780cde511ffaaf84cbdf9413.jpg', '1683268848252_1683268824978_jpg.rf.7e3bea2291ee66924ba2b0116e2a7c3c.jpg', '1683268849233_1683268825178_jpeg.rf.96ff74d832da0920bd059b4d5f230aba.jpg', 'IMG20230505114430_jpg.rf.c03b0bd9274b2b93c409d928b4c63a0b.jpg', '1-27-_jpg.rf.928a5ebfbe979ba5c03c64890c47cf4d.jpg', 'IMG20230505114042_jpg.rf.3f868c7995eade7eb39f5124f857f769.jpg', '1683268849267_1683268825185_jpg.rf.6dc2f40ce2ed00b212c9f2d7c2513770.jpg', 'IMG20230505114220_jpg.rf.4c0cd06fc365373174a7653e3bdb7229.jpg', '1-58-_jpg.rf.eaa9b274f402b5f03a9e653a131fb24c.jpg', 'IMG_4956_JPG.rf.ad35d7e6347c78833abef8ce9b6ab356.jpg', 'IMG20230505113240_jpg.rf.2d47ee6ea9ca40904a75ab2009bdce81.jpg', 'IMG20230505114038_jpg.rf.1f14c1c8a3a8711d2ad044c74976f421.jpg', 'IMG20230505114019_jpg.rf.b0f4a9a329ed75100b89fc2295afdedb.jpg', '1683268848174_1683268824957_jpg.rf.1b4833db29d514a51a9f0a039d8e233c.jpg', '1683268847975_1683268824891_jpg.rf.ed0fadd6e30c4d814d76f54dcd026136.jpg', 'IMG20230505114400_jpg.rf.8c06b64bb69acbe3d832a48f379bab8e.jpg', '1683268848019_1683268824908_jpg.rf.b31edcad193b2378f1d8bc47d026529a.jpg', '1683268849284_1683268825192_jpg.rf.5713683b78e1182e4f61642c31c4e6bd.jpg', '1683268848584_1683268825049_jpg.rf.c000c7dd95d032236068030e50864d73.jpg', 'IMG20230505114014_jpg.rf.a75311cbca62d3936ec46f1d5a7a8444.jpg', 'IMG20230505114340_jpg.rf.98f13d3b9763e1ed379aaa864ae74ff5.jpg', 'IMG20230505114109_jpg.rf.b9e467a234a18f720f8b84ba326ba1b7.jpg', 'IMG20230505113258_jpg.rf.0543e2801f2121f31516d064c6e72f04.jpg', 'IMG20230505114144_jpg.rf.49112e77a5e906a8aa6856210fbe14e8.jpg', '1-14-_jpg.rf.dc36f33c6d0c0deb77f754c5adc87a00.jpg', '1-84-_jpg.rf.a739e2205444fdb7abca7cf2d2967500.jpg', '1-24-_jpg.rf.fe327815bdf136c3f836f5de3784b485.jpg', 'IMG20230505114150_jpg.rf.75182a86eddcaeb322127b18eb591eca.jpg', '1-32-_jpg.rf.201196adfba65ebb2acc0da8f275fd8e.jpg', 'IMG20230505114346_jpg.rf.7db4fee7b3acba3030d7b9949cd82c24.jpg', 'IMG20230505113240_jpg.rf.7960dae6180f1356e47afef82cbdbad6.jpg', 'IMG20230505114518_jpg.rf.98f2c8f1da63214da66ae1b5ff7c288c.jpg', 'IMG20230505114246_jpg.rf.ea5ed05b3cfabadbf28b95c01f472fb4.jpg', '1-66-_jpg.rf.8f53d73c470b2856cf6473b97357b3be.jpg', '1-66-_jpg.rf.0b076d60ebc74070a20f83832087dc0d.jpg', 'IMG20230505114106_jpg.rf.fc58cf311a40027dc0b27b69a95f5f5e.jpg', '1683268849053_1683268825145_jpg.rf.f49ba8621e4589e244c533a62fb82a22.jpg', 'IMG20230505113347_jpg.rf.27c781713d75b7345828879784ff2192.jpg', '1-29-_jpg.rf.623526d6a60a5b8dd948367aa36f641e.jpg', 'IMG20230505114616_jpg.rf.03dbbb9e3e10783b2bed9a07c7b29dd2.jpg', 'IMG20230505112450_jpg.rf.309d3234ae2aa60f6b4dd8d47cfa134c.jpg', '1-20-_jpg.rf.b57a356a4d04254f1fb26322337b730d.jpg', '1-49-_jpg.rf.430cdcd3a47bce6cf23dff382ef403df.jpg', '1-23-_jpg.rf.a20a53b505e33f17818eeefd6bd749ad.jpg', 'IMG20230505110725_jpg.rf.4b94eb04ebf5b9e65ad742b21516bedd.jpg', 'IMG20230505113443_jpg.rf.9f704f86943dc475d571837297d49181.jpg', '1683268848421_1683268825016_jpg.rf.222461ac6d080e4e491159d6ead8047e.jpg', 'IMG20230505114010_jpg.rf.465770bf60f3f851acb08421d1b963af.jpg', '1683268848656_1683268825057_jpg.rf.9fd7b8e3bc0c3ffe80d21f80e100f42a.jpg', 'IMG20230505113301_jpg.rf.2e04963a77db6531132ebc7f089e6e51.jpg', 'IMG20230505114205_jpg.rf.e1a7d3074667d7f2fdf52f6766380608.jpg', 'IMG20230505113249_jpg.rf.7e285e922a961bab2765863768e17f21.jpg', 'IMG20230505114019_jpg.rf.de5c91fd81ecfc29e4dc6890472c1d08.jpg', 'IMG20230505113250_jpg.rf.ccb76c384299ec315d7fbca3f4d284b9.jpg', 'IMG20230505113347_jpg.rf.36b80d815a43df531e5348447fa7619a.jpg', 'IMG20230505114448_jpg.rf.2fb64f1862c8e10e9889f5bba7aa36c9.jpg', 'IMG20230505114514_jpg.rf.38cb883f45ad7a93181e47f3c9107b7b.jpg', '1-37-_jpg.rf.cfaff7a6135735fb7ac0de2287ac110e.jpg', 'IMG20230505114006_jpg.rf.5f7d06fe8cf4e345ae2f73439ceedb1c.jpg', '1-76-_jpg.rf.670a8f620c9a1d6741be6c2167383943.jpg', 'IMG_4779_JPG.rf.915530a83896162a651a9338f0e69cd8.jpg', 'IMG20230505113502_jpg.rf.878eb7bb9e39c495bd89153ed38fd4d1.jpg', '1683268849683_1683268825258_jpg.rf.c316c78a2e58f72a406e8a1411553092.jpg', 'IMG20230505114721_jpg.rf.cc4d13e8a4db23eff0a6ed4135881162.jpg', 'IMG20230505114229_jpg.rf.8f7b9cfcf45301dcec3c86f47640ad7d.jpg', '1683268849434_1683268825216_jpg.rf.4ea5e8271c9e28d714c4435a576a8f34.jpg', '1-42-_jpg.rf.286d1d92b016c192858f2560f2beeeb8.jpg', '1-24-_jpg.rf.9f105fbcbf7b9afbe0c26d59a50862e4.jpg', 'IMG20230505114554_jpg.rf.fcfc99c456bc2b5c3c85a9e27f45d331.jpg', 'IMG20230505114508_jpg.rf.89e1811b9a6bca5adfb9debd7992302c.jpg', 'IMG20230505114028_jpg.rf.a7526e2fab8d509d5bddda780640dd33.jpg', '1683268849617_1683268825242_jpg.rf.9bcd81f011f498bcfd7a15eed3a73594.jpg', 'IMG_4903_JPG.rf.d502daed172227524c68e76f13769e4a.jpg', '1-23-_jpg.rf.273a9bcaa10dd153ef9be995e9ec5834.jpg', 'IMG20230505113508_jpg.rf.3ab0c27db74a3046f5491086b17cd6a1.jpg', '1-71-_jpg.rf.8603c44b1bd289c024e6e6e565601c78.jpg', '1-82-_jpg.rf.c5a894d6b5297317a418e0f9932f8241.jpg', '1-15-_jpg.rf.f3cd31ff0f5554612232825d51e8fb1f.jpg', 'IMG20230505110744_jpg.rf.0523b6f3b4210c1dff4a69ad8e699489.jpg', '1-53-_jpg.rf.8a7a5fa233fb693f29f176561f620e73.jpg', 'IMG20230505114554_jpg.rf.991a8b559048cad5172d69395d8b2b97.jpg', '1-83-_jpg.rf.802999b99d9709f07c146eab6acec1a2.jpg', 'IMG20230505114458_jpg.rf.e6d578793c75a736343437e9ec601c41.jpg', 'IMG20230505114038_jpg.rf.672b0df7171c3750c329407b041ad567.jpg', 'IMG20230505114417_jpg.rf.2e43556500c82a0afe9e8edeeb6f8d9d.jpg', '1-58-_jpg.rf.63b707467680728c97c6306bc177b951.jpg', 'IMG20230505114215_jpg.rf.951cf473a761ca77704acb9992a13732.jpg', '1-76-_jpg.rf.556491eb352623d37cdf22ed51ddb334.jpg', 'IMG_4751_JPG.rf.4b04748e47de7a5e4d189fd441cf304e.jpg', '1683268849489_1683268825226_jpg.rf.803cf3b0363d8cf6c5e0259f45c463e2.jpg', 'IMG20230505114146_jpg.rf.aedca4ddaa540027dc0d8811daebcc92.jpg', 'IMG20230505114551_jpg.rf.6f3ae7f0e8874697052ffa1ecb1c3dc9.jpg', 'IMG20230505113538_jpg.rf.492d84c79cd0545c9d1e7df114a92f3a.jpg', '1-91-_jpg.rf.db59108219d8ca874b708b244a1a395d.jpg', 'IMG20230505114417_jpg.rf.6874a70423af831e83e1f5f35dd5dfd5.jpg', '1683268848212_1683268824968_jpg.rf.3b8f2b903468eb61bdd0e4e335b59dc0.jpg', '1683268848844_1683268825100_jpg.rf.d6d26fd5da34738f618a42cdd089d75a.jpg', 'IMG20230505114150_jpg.rf.42955330db338a8bab2dece60a892862.jpg', 'IMG20230505110710_jpg.rf.c81b72e0111970159f74b1dc4172bd70.jpg', 'IMG20230505113337_jpg.rf.2ef5fa0eec7dbd9dd144f0957c9fc725.jpg', 'IMG20230505113321_jpg.rf.24e5f7411b7d9d0f2ee51298a643b1ab.jpg', 'IMG_4961_JPG.rf.afa1e799ad0e22da93df4d205edf7638.jpg', '1-23-_jpg.rf.d87bac637c3925ec4fde09d7f1726c4d.jpg', '1683268849008_1683268825136_jpg.rf.dc277f71a0cfcff3856a43efefe60b8c.jpg', '1683268848090_1683268824933_jpg.rf.0ad44902957d7b50769e410806898899.jpg', 'IMG20230505114758_jpg.rf.1c4167342bd3abecb0b3d7b7fe1a3ba5.jpg', 'IMG_4903_JPG.rf.2e2789a6d8942e1684acf77323be7aee.jpg', '1-81-_jpg.rf.cd5b494b573f5cb2a9a8eb92490cb8e2.jpg', 'IMG20230505114229_jpg.rf.1205dca29c406788732c4546f7f0525d.jpg', '1-32-_jpg.rf.b38188ca28ec0ee04c927ea281ffb5cf.jpg', 'IMG20230505113505_jpg.rf.865416da15b514a16a769298de4f61a0.jpg', 'IMG20230505114104_jpg.rf.f3442398cdeafe71c138854dfbdb5bc1.jpg', 'IMG_4950_JPG.rf.207dce57f24a37733d5b75ab572bc645.jpg', 'IMG20230505114506_jpg.rf.d69a3ab344e733bdf2683a06740b2e10.jpg', '1-76-_jpg.rf.7f5913ce3f108b7b505133eb3db681c4.jpg', '1-84-_jpg.rf.e419fce2096400cac915778b340dd623.jpg', '1-87-_jpg.rf.99124c52acc73dec1447e4aab2d5f7e3.jpg', 'IMG20230505112658_jpg.rf.3421d1c5432d75e13cc6e683ca05dcd1.jpg', 'IMG20230505114259_jpg.rf.d729cedf692d907d1af6920b5d8ff654.jpg', 'IMG20230505114158_jpg.rf.cbb8ec8e4f4696454e7726158923ff4a.jpg', 'IMG20230505113335_jpg.rf.26a4e38e7b190f6f5df0c1bd508b7670.jpg', '1683268848421_1683268825016_jpg.rf.86bb8edca05e19ef033e1b229bebeac9.jpg', '1-5-_jpg.rf.88d17c5fb3127fd6b603a64898186a91.jpg', '1-79-_jpg.rf.77ca537ca742663be8f23667ba79df77.jpg', 'IMG20230505113404_jpg.rf.b24a5894269e62deb3e44253dec24078.jpg', 'IMG20230505114346_jpg.rf.dcd8fd561da7b0ea51454ad8f7bb747c.jpg', '1-88-_jpg.rf.ff9c0c2b72cc66f6250ecffd13465eab.jpg', 'IMG20230505113235_jpg.rf.33e081c4f32e50826136feb36bf1a82b.jpg', 'IMG20230505112658_jpg.rf.6968a7252884143657def223279c847c.jpg', '1683268849649_1683268825250_jpg.rf.b4230b3a97122f6b08e5e228cc81616d.jpg', 'IMG20230505113311_jpg.rf.e9bee64a3f630bab4d990dccb49aee83.jpg', 'IMG20230505114105_jpg.rf.eb3947b6c1679e195f0499e9de0facc3.jpg', 'IMG20230505114514_jpg.rf.7f33826c29c90b2a4995923d32e74cde.jpg', '1683268848212_1683268824968_jpg.rf.f929596b8509248d0030ef0aebc2ec87.jpg', 'IMG20230505114104_jpg.rf.de340b18a6b0b620ab63f9561082a2a3.jpg', 'IMG20230505114003_jpg.rf.4db6b42caf5f8f0180fd8bce1d653428.jpg', '1683268849053_1683268825145_jpg.rf.4de847c692e2969223eb94821f58c05f.jpg', '1683268849587_1683268825233_jpg.rf.ba3a0a050ef7c9a6a903597540e7a60a.jpg', '1683268848844_1683268825100_jpg.rf.4ec855e3d4714b7f69c2f951ae08d8ba.jpg', 'IMG_4854_JPG.rf.735b0cadfda7d1850bf3d7a7be1b2dca.jpg', 'IMG20230505113314_jpg.rf.42fcb404048b196842dc7b17cd16a903.jpg', 'IMG_4869_JPG.rf.01de88f47861c8e66cf2b32c6d7971bd.jpg', 'IMG20230505114606_jpg.rf.219a47b3bb2c7b9cb61d45cb9030a953.jpg', 'IMG20230505114031_jpg.rf.53a53de2601030a1a5ee9d73b9c6b962.jpg', 'IMG20230505114319_jpg.rf.217bb1394592ab8dcf1e9f0c1e72aaf9.jpg', 'IMG20230505110737_jpg.rf.f2dede7533bbfe902a2018a29bac9b89.jpg', 'IMG20230505114506_jpg.rf.c4b3e58fc24ba99dca71ccdc3f1acb70.jpg', '1-8-_jpg.rf.fbf1bec5b1bd363594893692cae05ebc.jpg', 'IMG20230505114636_jpg.rf.ab108b8115ba1338d309e1fc0e6a8d74.jpg', 'IMG20230505114124_jpg.rf.b5b85bd65c3358d982166b3b3ec3bf26.jpg', '1-12-_jpg.rf.eefd99011d6159247467a37dff7b334d.jpg', '1-21-_jpg.rf.7c20f4fea0e8e00b12ae1aa3737d6b34.jpg', 'IMG20230505114106_jpg.rf.7f99cf3f072b60947050cd0c08309686.jpg', '1-79-_jpg.rf.6dce19ec0f6f254e3c4444899986b56b.jpg', '1-74-_jpg.rf.9f251c21025e0dbb4721fb9d52429599.jpg', 'IMG20230505114223_jpg.rf.a1931ae0f1a2d6068965efedc8e37274.jpg', 'IMG20230505114458_jpg.rf.39d53c5196b6c0d6e2975f47be98292b.jpg', '1-49-_jpg.rf.e8a41561954c728729621491fd75c2cf.jpg', 'IMG_6838-2_jpg.rf.afad8d174d65300b04542cfe197cd5d1.jpg', '1-88-_jpg.rf.76b9955d3d04af744973a43207e3dcac.jpg', '1-62-_jpg.rf.d668c41aef2bd08ba57f3f3623a303a5.jpg', '1683268848381_1683268825008_jpg.rf.dc13ab0c9810d749ef7eb832417198e8.jpg', 'IMG20230505114006_01_jpg.rf.2e02ec0c5cf01a3a7b4736c7f7702d2e.jpg', 'IMG20230505114657_jpg.rf.3b507ff226e94dd3480dbd2e0af35f59.jpg', 'IMG20230505114721_jpg.rf.aadfffc2237b9b6eebfac0f8dc217339.jpg', '1-6-_jpg.rf.ff84ad182181662b34fa85a8a5fcb6ad.jpg', 'IMG20230505114143_jpg.rf.88af033f39c5cfb5c9deaf3d36342640.jpg', '1-19-_jpg.rf.3f814bb690a3f930aaaca940172b41cb.jpg', 'IMG20230505114209_jpg.rf.739c1ed679f9f7e495274a37b8a55a44.jpg', '1-17-_jpg.rf.d74baa199ff061f35fdba7fb9d13c98d.jpg', 'IMG20230505114518_jpg.rf.4abdf7677e8033344908057595fd3f60.jpg', 'IMG20230505114421_jpg.rf.544f28166f0bc4e3aa3858e11f991744.jpg', '1683268849284_1683268825192_jpg.rf.a20f8f5b0e227012fa5bcbd65fb0a5b0.jpg', '1-28-_jpg.rf.0969f37fce7ef80344c565ecfa668f55.jpg', '1683268848280_1683268824992_jpg.rf.50bca2c3b1f5f7ac07b7acb9ef1217a7.jpg', '1-4-_jpg.rf.27d6730d07177190e4afa61194d52db2.jpg', 'IMG20230505114209_jpg.rf.1eab7874d37c9c1c7154a28b493096a7.jpg', '1-22-_jpg.rf.981c440327ebbe28ce6adab8b166dfde.jpg', '1683268848019_1683268824908_jpg.rf.2f45a9021f7c6db65db33e98f53bc87e.jpg', 'IMG20230505113235_jpg.rf.b58b3369cb23c46e480b2efe64515b68.jpg', '1683268849683_1683268825258_jpg.rf.9fd255fb575c556e71cbb1366aaca683.jpg', '1-91-_jpg.rf.57715fd636ae6acbd136004d90ad24f9.jpg', '1-6-_jpg.rf.2c4faac13482c171b9d03653362ccbf4.jpg', '1-79-_jpg.rf.fe96fe23f000a9ca0f73a213737cd624.jpg', 'IMG20230505114046_jpg.rf.690cb77877fa61c608baaec9e14dedd2.jpg', 'IMG20230505114002_jpg.rf.66978bd431bca3ce950b06c356a88abf.jpg', 'IMG20230505113249_jpg.rf.20995790d79a04559c958257dafd8ee5.jpg', '1683268849683_1683268825258_jpg.rf.a0d14c1e6253d45b62eb1d051bcd4d4a.jpg', '1683268848338_1683268825000_jpg.rf.fa252e37fea4c42a997d15010a819517.jpg', 'IMG20230505114522_jpg.rf.365216405860c19ad314b2bad21d55f9.jpg', '1-73-_jpg.rf.0aeb25c7049f27748ffae609df9056bd.jpg', 'IMG20230505114124_jpg.rf.ae06faa5c40d7a115c4c6614c4a25858.jpg', 'IMG_4751_JPG.rf.691217995fbece422c22e69e2eb2b3d2.jpg', 'IMG20230505114014_jpg.rf.77a2eb25bae3d45697ff032b09b8879e.jpg', 'IMG20230505110725_jpg.rf.00514f175e40bfa17913dcf15e0f6da7.jpg', '1-55-_jpg.rf.06935c81aef0283f6e95d3b45bc72f13.jpg', 'IMG20230505114212_jpg.rf.a68402ff16a2890b25d4e6a52b97c1c3.jpg', '1-54-_jpg.rf.03812e85a6474d1a665e6817aae3b01f.jpg', 'IMG20230505113307_jpg.rf.e06ff90285b680101c25b6ad8e284af7.jpg', 'IMG_4961_JPG.rf.4a857f77ff9baf9f5e081cb35db80ef3.jpg', '1-54-_jpg.rf.8ea9f12bcc1fff5d3fbdc35e71b07688.jpg', '1683268849117_1683268825161_jpg.rf.605e99a0119bbc2583b8cbdaa382d7a7.jpg', '1683268848338_1683268825000_jpg.rf.1553de032cf08de470496987455308a1.jpg', 'IMG20230505114058_jpg.rf.6605b4da4d892079b89f12db5747903e.jpg', 'IMG20230505114104_jpg.rf.f6929f6e089ce11b35734676965f0f5f.jpg', '1-47-_jpg.rf.95bd97112325f55c528fe4c7d6b90708.jpg', '1683268848947_1683268825128_jpg.rf.2923bd5bebc95fa2f101a56092eb9943.jpg', '1683268848252_1683268824978_jpg.rf.4c5ee78007b8690e809c40a5a67d1699.jpg', 'IMG20230505114130_jpg.rf.798547ae69d2e99dc5365e8f7d434cd4.jpg', 'IMG20230505110818_jpg.rf.a7936fae5694e99d7d2aa3474c1805d2.jpg', '1683268848019_1683268824908_jpg.rf.fe32b787b25ca870ebfd423e4f77ce25.jpg', 'IMG20230505113246_jpg.rf.ef983473048f45d3517267f1112e4afc.jpg', 'IMG20230505113540_jpg.rf.a459ed91c3925c540becae818733ed53.jpg', '1-75-_jpg.rf.4a358e1619ca96a3b10c092330d9aceb.jpg', '1-47-_jpg.rf.c243021238d7d9a1712492a4fc373fab.jpg', 'IMG20230505114158_jpg.rf.4234921334e2d299c9b9a8952ab894fc.jpg', '1683268848656_1683268825057_jpg.rf.79f7cc333ff46e4d92da953a9972a438.jpg', 'IMG20230505113339_jpg.rf.ac66c323544ef4d3ee67a23744f48a1a.jpg', '1683268848252_1683268824978_jpg.rf.696e48af57369074a1aa11ceae920dda.jpg', '1-47-_jpg.rf.9658e7db675151a2b2b96addda96711e.jpg', 'IMG20230505113957_jpg.rf.f8b1157058c7f22ff845abaf871fe011.jpg', 'IMG20230505114146_jpg.rf.ced9c4662666d88b781759629d1e3d09.jpg', '1-56-_jpg.rf.1bf8564a58309495651e90c57667e694.jpg', 'IMG20230505114351_jpg.rf.f40eba6544dcc8a64d79c4a800a8fe93.jpg', '_annotations.coco.json', 'IMG20230505114351_jpg.rf.40e091a14b3461c7a93ce78ba0f71f89.jpg', '1683268848131_1683268824947_jpg.rf.cc53136123dcc8d66a0ba5ad4a28fbaf.jpg', 'IMG20230505113434_jpg.rf.fa493cd811a8dbb11fa4a8c2779c219a.jpg', '1-59-_jpg.rf.60d841cb09b88a7353fc81692afb1dfe.jpg', '1-5-_jpg.rf.5140fe6e206feb7db71341ed8eb34b50.jpg', 'IMG20230505114022_jpg.rf.7ba7927465b923b1944f6f4c255ceafd.jpg', 'IMG20230505113456_jpg.rf.5278b85d1e3b0494464937d793ce186b.jpg', '1683268849117_1683268825161_jpg.rf.39abef9f7de88e29aaf9ebe7befd6bc1.jpg', '1683268849617_1683268825242_jpg.rf.f35a3e241b0330907c50eab2f41b6534.jpg', 'IMG20230505114146_jpg.rf.707a8920c4f1d7e423fc6c6043d62b40.jpg', 'IMG20230505114010_jpg.rf.ef988fc33c95b2d59206103d138598b0.jpg', 'IMG_4779_JPG.rf.9382612e05355e1fc79422d34830d783.jpg', '1-64-_jpg.rf.585f7d237330e8fe1d063b4a36367591.jpg', '1-40-_jpg.rf.3c53aca6a91e61296434ec8f2c3dde6b.jpg', 'IMG20230505114324_jpg.rf.92bba0c896874522e569c53687f68bda.jpg', 'IMG20230505113250_jpg.rf.61be3f0f7328700f7a9606c862c389a7.jpg', '1-83-_jpg.rf.fa247d7649eed022a9971fd5d1526385.jpg', 'IMG20230505114205_jpg.rf.3416c92ed5883ad9d0f4632ab469ba0e.jpg', 'IMG20230505114206_jpg.rf.0187bf7daa075c5bd438705b758531a0.jpg', 'IMG20230505113246_jpg.rf.56c671cdd4f26f3820c9fe93fcb82939.jpg', '1683268848174_1683268824957_jpg.rf.1c32ec14df9110702e27eca26d32ac34.jpg', '1-19-_jpg.rf.fd24b27bf6c9fca35d8336561139b859.jpg', '1-49-_jpg.rf.5705f2e084ed24fb54602566cb892106.jpg', 'IMG20230505114215_jpg.rf.03490589216d7b1e6b2230c386214725.jpg', '1683268849587_1683268825233_jpg.rf.5ad3c4c8be300e584f1e423a4cfc4629.jpg', '1-46-_jpg.rf.33261b67281c458b96c0f48389d5c1c8.jpg', 'IMG20230505114549_jpg.rf.559c434f45325d921cc2c8e8f8a46558.jpg', 'IMG20230505114037_jpg.rf.0717798ffcfbb91a107feaaf76b44640.jpg', '1-8-_jpg.rf.4f0fe513dd5d708df1b7ee975087110b.jpg', '1683268848584_1683268825049_jpg.rf.1df28e80ce9c10be15c9a67638c4d51b.jpg', 'IMG20230505114002_jpg.rf.102c960059c5423710d1d3d4b07c61c7.jpg', 'IMG20230505110717_jpg.rf.cb84e80352f258091b7aa3f93d603982.jpg', '1-27-_jpg.rf.1889e8a9bd6f2ef18895ad74904d89b1.jpg', 'IMG20230505110818_jpg.rf.580c36caaf94ebdb0524702ff555943c.jpg', 'IMG20230505113250_jpg.rf.8ff98265399a18ebe5ff992ed98e0864.jpg', 'IMG_4854_JPG.rf.175eb173790e18fb1c6a86f1cee93a34.jpg', 'IMG20230505114249_jpg.rf.f1fa06e7f5c2b64153eb0ee6c1b6f7b8.jpg', 'IMG20230505114552_jpg.rf.30eafbfb9b65808cca9d2f46897bd941.jpg', '1-54-_jpg.rf.8c8d03e2bfa408e178ac772bd1c48eb7.jpg', 'IMG20230505113502_jpg.rf.1c74f57f8a9f0ab74399ca901fa5f319.jpg', '1-53-_jpg.rf.ba2413ab5ec9e4fa6d87379287a0810b.jpg', 'IMG20230505114421_jpg.rf.b6231b4241cf26e664b84953c5d50e2d.jpg', 'IMG20230505114448_jpg.rf.1023e23f9437de5729ad07812957cb60.jpg', 'IMG20230505110744_jpg.rf.940fea2a37f7005b20103ad61c21ff64.jpg', 'IMG20230505114552_jpg.rf.4bdeb8bc12980fd131cc1b6d36c3a5e6.jpg', 'IMG20230505114042_jpg.rf.0c8cbf7359a64ee1cc8a02e73a75c415.jpg', 'IMG_4903_JPG.rf.6166c9c1298e805bfde1d272f7ecdfcb.jpg', 'IMG20230505114348_jpg.rf.afa0d48c135a2d894010086e933cbd58.jpg', 'IMG_4869_JPG.rf.8752f37928faaddd243e0d52f92f90d4.jpg', '1683268849714_1683268825263_jpg.rf.591fb8ee6ac5ad3b8a1a6e2531f29228.jpg', '1-62-_jpg.rf.d22c69c445ea5e91e4f5deeaf11f9ac6.jpg', 'IMG20230505114552_jpg.rf.e9edb8dd4447b79bd04ad81a5169f9d0.jpg', 'IMG20230505114206_jpg.rf.898788bfe49e718e09074ef383154b04.jpg', 'IMG20230505114739_jpg.rf.0126c962d47e002685d5fc53748498bc.jpg', 'IMG20230505114319_jpg.rf.d066c9a408a13bb4e984c06b8ce4bfee.jpg', '1-12-_jpg.rf.fda1c80d0e3235f43d9d0b8c06eddbed.jpg', '1-39-_jpg.rf.dd56fe2bca5f2bbfc65debd03cf3adaf.jpg', '1-53-_jpg.rf.8c4d82339b4420db9ece1fc3274e2640.jpg', '1-63-_jpg.rf.dc07af074660c73fd32785b07d057750.jpg', '1683268849117_1683268825161_jpg.rf.16e554a006af1b84cf59cb7149822129.jpg', 'IMG20230505113540_jpg.rf.02aec882f10376bedf0553f40c31c1c6.jpg', '1683268848280_1683268824992_jpg.rf.bc32048edadc26dfa99976bce23cf21a.jpg', '1683268847975_1683268824891_jpg.rf.ba4dd89fb76e9dce180911730b6607af.jpg', 'IMG20230505113540_jpg.rf.cbf9e95d8f04c3fd4fd321fb214a7110.jpg', 'IMG20230505113508_jpg.rf.5bd627fb1f910b661cbd1e3e02ef973f.jpg', 'IMG20230505114022_jpg.rf.74d7df127a8844626c990c12540bb17b.jpg', 'IMG20230505113443_jpg.rf.93d9e430cc304c8e82fe2fa2b42d9f63.jpg', 'IMG20230505114421_jpg.rf.f45d8866ad25a3478dbfdab72ac00f31.jpg', 'IMG20230505113434_jpg.rf.c616eef9a8a19bb6a5e4a14e2e1b02a3.jpg', 'IMG20230505113301_jpg.rf.6733424d629a4875cd1f54ce78f5066e.jpg', 'IMG20230505114037_jpg.rf.d7cb0840472164505a19c9864ed34fac.jpg', '1-52-_jpg.rf.722d9f330ec58c07c8259a5fbcc677e0.jpg', 'IMG20230505110757_jpg.rf.defdb1d11dcf3a47cb83c57c02a6da7e.jpg', '1-22-_jpg.rf.9e5dbba8d48eda21ede02761e8cd019f.jpg', '1683268849233_1683268825178_jpeg.rf.ed94128372d44967e0701e4f6f13d6a3.jpg', 'IMG20230505114132_jpg.rf.abd0688c8921d1c08d73061c7e3e7b1c.jpg', 'IMG20230505114549_jpg.rf.1d95b1918a84c86333b7041284d2ae32.jpg', '1-29-_jpg.rf.d4e33ecc67de0bad6a161d5d00c283e2.jpg', '1683268849267_1683268825185_jpg.rf.6623bcce696569c64dd3d749fdeb2114.jpg', 'IMG20230505114348_01_jpg.rf.c4d8cfebfa23ccfe5b9d37e033cf945b.jpg', '1683268848174_1683268824957_jpg.rf.110843018a1934c186df539777693c5b.jpg', 'IMG20230505114106_jpg.rf.bfebf1a3003dea26c74245009ac6d9ad.jpg', 'IMG20230505114739_jpg.rf.446d65a964643bca7d115e592d847b4c.jpg', 'IMG20230505114756_jpg.rf.d7f7168eb6aeea27430e36179ea46a6e.jpg', 'IMG20230505114003_jpg.rf.ae2a065fc026e8c9e499ff5f5d108c2c.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_dir = \"/content/Attendance-System-Using-Face--2\"\n",
    "\n",
    "print(\"Root contents:\", os.listdir(root_dir))\n",
    "print(\"Train contents:\", os.listdir(os.path.join(root_dir, \"train\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac25fa9",
   "metadata": {},
   "source": [
    "### Crop faces from COCO for all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb188e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Found 633 images, 1857 face instances\n",
      "[train] Saved 1857 cropped faces to: /content/Attendance-System-Using-Face--2/cropped_faces_train\n",
      "[valid] Found 62 images, 183 face instances\n",
      "[train] Saved 1857 cropped faces to: /content/Attendance-System-Using-Face--2/cropped_faces_train\n",
      "[valid] Found 62 images, 183 face instances\n",
      "[valid] Saved 183 cropped faces to: /content/Attendance-System-Using-Face--2/cropped_faces_valid\n",
      "[test] Found 31 images, 97 face instances\n",
      "[valid] Saved 183 cropped faces to: /content/Attendance-System-Using-Face--2/cropped_faces_valid\n",
      "[test] Found 31 images, 97 face instances\n",
      "[test] Saved 97 cropped faces to: /content/Attendance-System-Using-Face--2/cropped_faces_test\n",
      "[test] Saved 97 cropped faces to: /content/Attendance-System-Using-Face--2/cropped_faces_test\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "root_dir = Path(\"/content/Attendance-System-Using-Face--2\")\n",
    "\n",
    "def crop_split(split_name: str):\n",
    "    split_dir = root_dir / split_name\n",
    "    ann_path = split_dir / \"_annotations.coco.json\"\n",
    "\n",
    "    if not ann_path.exists():\n",
    "        print(f\"[{split_name}] No COCO annotation file found at {ann_path}, skipping.\")\n",
    "        return\n",
    "\n",
    "    out_dir = root_dir / f\"cropped_faces_{split_name}\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    with open(ann_path, \"r\") as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    images = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "    annotations = coco[\"annotations\"]\n",
    "\n",
    "    print(f\"[{split_name}] Found {len(images)} images, {len(annotations)} face instances\")\n",
    "\n",
    "    counter = 0\n",
    "    for ann in annotations:\n",
    "        img_id = ann[\"image_id\"]\n",
    "        img_info = images[img_id]\n",
    "        file_name = img_info[\"file_name\"]\n",
    "\n",
    "        img_path = split_dir / file_name\n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "\n",
    "        # COCO bbox = [x, y, width, height]\n",
    "        x, y, w, h = ann[\"bbox\"]\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "\n",
    "        with Image.open(img_path) as im:\n",
    "            # clip to image bounds\n",
    "            x2 = min(x + w, im.width)\n",
    "            y2 = min(y + h, im.height)\n",
    "            x = max(x, 0)\n",
    "            y = max(y, 0)\n",
    "\n",
    "            if x2 <= x or y2 <= y:\n",
    "                continue\n",
    "\n",
    "            face_crop = im.crop((x, y, x2, y2))\n",
    "\n",
    "            out_file = out_dir / f\"face_{counter:06d}.jpg\"\n",
    "            face_crop.save(out_file)\n",
    "            counter += 1\n",
    "\n",
    "    print(f\"[{split_name}] Saved {counter} cropped faces to: {out_dir}\")\n",
    "\n",
    "# Run for train / valid / test\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    crop_split(split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae53de36",
   "metadata": {},
   "source": [
    "### Confirm cropped faces exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb82f7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped train faces: 1857\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Cropped train faces:\", len(os.listdir(\"/content/Attendance-System-Using-Face--2/cropped_faces_train\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a54fb1",
   "metadata": {},
   "source": [
    "### Data Preparation & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf6de42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dir exists: True\n",
      "Valid dir exists: True\n",
      "Test dir exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/content/Attendance-System-Using-Face--2\")\n",
    "\n",
    "cropped_train = ROOT / \"cropped_faces_train\"\n",
    "cropped_valid = ROOT / \"cropped_faces_valid\"\n",
    "cropped_test  = ROOT / \"cropped_faces_test\"\n",
    "\n",
    "print(\"Train dir exists:\", cropped_train.exists())\n",
    "print(\"Valid dir exists:\", cropped_valid.exists())\n",
    "print(\"Test dir exists:\",  cropped_test.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7765d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Requirement already satisfied: dlib in /usr/local/lib/python3.12/dist-packages (19.24.6)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python pillow dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc0220",
   "metadata": {},
   "source": [
    "### Defining Blurring & Noising Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab3de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "def add_gaussian_blur(img, ksize=5):\n",
    "    # ksize must be odd: 3,5,7,...\n",
    "    return cv2.GaussianBlur(img, (ksize, ksize), 0)\n",
    "\n",
    "def add_gaussian_noise(img, mean=0, var=20):\n",
    "    # img: uint8 BGR\n",
    "    sigma = var ** 0.5\n",
    "    gauss = np.random.normal(mean, sigma, img.shape).reshape(img.shape).astype(np.float32)\n",
    "    noisy = img.astype(np.float32) + gauss\n",
    "    noisy = np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "    return noisy\n",
    "\n",
    "def add_salt_pepper_noise(img, amount=0.01):\n",
    "    # amount = fraction of pixels to corrupt\n",
    "    noisy = img.copy()\n",
    "    num_pixels = img.shape[0] * img.shape[1]\n",
    "    num_salt = int(amount * num_pixels / 2)\n",
    "    num_pepper = int(amount * num_pixels / 2)\n",
    "\n",
    "    # Salt (white)\n",
    "    coords = (\n",
    "        np.random.randint(0, img.shape[0], num_salt),\n",
    "        np.random.randint(0, img.shape[1], num_salt),\n",
    "    )\n",
    "    noisy[coords] = 255\n",
    "\n",
    "    # Pepper (black)\n",
    "    coords = (\n",
    "        np.random.randint(0, img.shape[0], num_pepper),\n",
    "        np.random.randint(0, img.shape[1], num_pepper),\n",
    "    )\n",
    "    noisy[coords] = 0\n",
    "\n",
    "    return noisy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aeb169",
   "metadata": {},
   "source": [
    "### Augment all cropped faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199eab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Found 1857 images in /content/Attendance-System-Using-Face--2/cropped_faces_train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Found 1857 images in /content/Attendance-System-Using-Face--2/cropped_faces_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting train: 100%|██████████| 1857/1857 [00:10<00:00, 171.61it/s]\n",
      "Augmenting train: 100%|██████████| 1857/1857 [00:10<00:00, 171.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Found 1857 images in /content/Attendance-System-Using-Face--2/cropped_faces_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting train: 100%|██████████| 1857/1857 [00:10<00:00, 171.61it/s]\n",
      "Augmenting train: 100%|██████████| 1857/1857 [00:10<00:00, 171.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Augmentation done. Check: /content/Attendance-System-Using-Face--2/aug_faces_train\n",
      "[valid] Found 183 images in /content/Attendance-System-Using-Face--2/cropped_faces_valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting valid: 100%|██████████| 183/183 [00:01<00:00, 152.16it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Found 1857 images in /content/Attendance-System-Using-Face--2/cropped_faces_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting train: 100%|██████████| 1857/1857 [00:10<00:00, 171.61it/s]\n",
      "Augmenting train: 100%|██████████| 1857/1857 [00:10<00:00, 171.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Augmentation done. Check: /content/Attendance-System-Using-Face--2/aug_faces_train\n",
      "[valid] Found 183 images in /content/Attendance-System-Using-Face--2/cropped_faces_valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting valid: 100%|██████████| 183/183 [00:01<00:00, 152.16it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] Augmentation done. Check: /content/Attendance-System-Using-Face--2/aug_faces_valid\n",
      "[test] Found 97 images in /content/Attendance-System-Using-Face--2/cropped_faces_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting test: 100%|██████████| 97/97 [00:00<00:00, 137.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Found 1857 images in /content/Attendance-System-Using-Face--2/cropped_faces_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting train: 100%|██████████| 1857/1857 [00:10<00:00, 171.61it/s]\n",
      "Augmenting train: 100%|██████████| 1857/1857 [00:10<00:00, 171.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Augmentation done. Check: /content/Attendance-System-Using-Face--2/aug_faces_train\n",
      "[valid] Found 183 images in /content/Attendance-System-Using-Face--2/cropped_faces_valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting valid: 100%|██████████| 183/183 [00:01<00:00, 152.16it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] Augmentation done. Check: /content/Attendance-System-Using-Face--2/aug_faces_valid\n",
      "[test] Found 97 images in /content/Attendance-System-Using-Face--2/cropped_faces_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting test: 100%|██████████| 97/97 [00:00<00:00, 137.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] Augmentation done. Check: /content/Attendance-System-Using-Face--2/aug_faces_test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Found 1857 images in /content/Attendance-System-Using-Face--2/cropped_faces_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting train: 100%|██████████| 1857/1857 [00:10<00:00, 171.61it/s]\n",
      "Augmenting train: 100%|██████████| 1857/1857 [00:10<00:00, 171.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Augmentation done. Check: /content/Attendance-System-Using-Face--2/aug_faces_train\n",
      "[valid] Found 183 images in /content/Attendance-System-Using-Face--2/cropped_faces_valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting valid: 100%|██████████| 183/183 [00:01<00:00, 152.16it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] Augmentation done. Check: /content/Attendance-System-Using-Face--2/aug_faces_valid\n",
      "[test] Found 97 images in /content/Attendance-System-Using-Face--2/cropped_faces_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting test: 100%|██████████| 97/97 [00:00<00:00, 137.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] Augmentation done. Check: /content/Attendance-System-Using-Face--2/aug_faces_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_split(split_name: str):\n",
    "    in_dir = ROOT / f\"cropped_faces_{split_name}\"\n",
    "    if not in_dir.exists():\n",
    "        print(f\"[{split_name}] Input dir not found, skipping:\", in_dir)\n",
    "        return\n",
    "\n",
    "    out_dir = ROOT / f\"aug_faces_{split_name}\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    images = [f for f in os.listdir(in_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    print(f\"[{split_name}] Found {len(images)} images in {in_dir}\")\n",
    "\n",
    "    counter = 0\n",
    "    for fname in tqdm(images, desc=f\"Augmenting {split_name}\"):\n",
    "        in_path = in_dir / fname\n",
    "        img = cv2.imread(str(in_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        # Save original copy\n",
    "        base_name = Path(fname).stem\n",
    "        ext = Path(fname).suffix\n",
    "\n",
    "        cv2.imwrite(str(out_dir / f\"{base_name}_orig{ext}\"), img)\n",
    "\n",
    "        # Blurs with different kernel sizes\n",
    "        for k in [3, 5, 7]:\n",
    "            blurred = add_gaussian_blur(img, ksize=k)\n",
    "            cv2.imwrite(str(out_dir / f\"{base_name}_blur{k}{ext}\"), blurred)\n",
    "\n",
    "        # Gaussian noise with different variances\n",
    "        for v in [10, 20, 30]:\n",
    "            noisy = add_gaussian_noise(img, var=v)\n",
    "            cv2.imwrite(str(out_dir / f\"{base_name}_gn{v}{ext}\"), noisy)\n",
    "\n",
    "        # Salt & pepper noise\n",
    "        for amt in [0.01, 0.02]:\n",
    "            sp = add_salt_pepper_noise(img, amount=amt)\n",
    "            cv2.imwrite(str(out_dir / f\"{base_name}_sp{int(amt*100)}{ext}\"), sp)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    print(f\"[{split_name}] Augmentation done. Check:\", out_dir)\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    augment_split(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3edd9cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-03 09:57:14--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
      "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
      "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 [following]\n",
      "--2025-12-03 09:57:14--  https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
      "Connecting to dlib.net (dlib.net)|107.180.26.78|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64040097 (61M)\n",
      "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
      "\n",
      "shape_predictor_68_ 100%[===================>]  61.07M  25.2MB/s    in 2.4s    \n",
      "\n",
      "2025-12-03 09:57:17 (25.2 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
    "!mkdir -p /content/models\n",
    "!mv shape_predictor_68_face_landmarks.dat /content/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4919d4a5",
   "metadata": {},
   "source": [
    "### DLib Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaddcf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlib landmark model loaded successfully \n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "\n",
    "shape_model_path = \"/content/models/shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(shape_model_path)\n",
    "\n",
    "print(\"dlib landmark model loaded successfully \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e67ef",
   "metadata": {},
   "source": [
    "### Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1732e1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dir: /content/Attendance-System-Using-Face--2/cropped_faces_train exists: True\n",
      "Valid dir: /content/Attendance-System-Using-Face--2/cropped_faces_valid exists: True\n",
      "Test dir: /content/Attendance-System-Using-Face--2/cropped_faces_test exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/content/Attendance-System-Using-Face--2\")\n",
    "\n",
    "cropped_train = ROOT / \"cropped_faces_train\"\n",
    "cropped_valid = ROOT / \"cropped_faces_valid\"\n",
    "cropped_test  = ROOT / \"cropped_faces_test\"\n",
    "\n",
    "print(\"Train dir:\", cropped_train, \"exists:\", cropped_train.exists())\n",
    "print(\"Valid dir:\", cropped_valid, \"exists:\", cropped_valid.exists())\n",
    "print(\"Test dir:\",  cropped_test,  \"exists:\", cropped_test.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8fdd1",
   "metadata": {},
   "source": [
    "### Define face alignment function using dlib landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "147dc1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlib ERT landmark model loaded \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT = Path(\"/content/Attendance-System-Using-Face--2\")\n",
    "\n",
    "# Path to your Kazemi-style landmark model\n",
    "shape_model_path = \"/content/models/shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(shape_model_path)\n",
    "\n",
    "print(\"dlib ERT landmark model loaded \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3819736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_face_dlib(img_bgr, desired_size=160, desired_left_eye=(0.35, 0.35)):\n",
    "    \"\"\"\n",
    "    Aligns a face using dlib's 68-point landmarks (Kazemi & Sullivan ERT)\n",
    "    and an affine transform based on eye positions.\n",
    "\n",
    "    img_bgr: OpenCV image (BGR)\n",
    "    returns: aligned face (BGR) or None if no face found\n",
    "    \"\"\"\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    dets = detector(img_rgb, 1)\n",
    "\n",
    "    if len(dets) == 0:\n",
    "        return None\n",
    "\n",
    "    # Pick the largest detected face (in case there are multiple)\n",
    "    rect = max(dets, key=lambda r: r.width() * r.height())\n",
    "    shape = shape_predictor(img_rgb, rect)\n",
    "\n",
    "    # 68 landmarks → numpy\n",
    "    landmarks = np.array([[p.x, p.y] for p in shape.parts()], dtype=np.float32)\n",
    "\n",
    "    # Eye landmark indices for the 68-point model\n",
    "    left_eye_pts  = landmarks[36:42]\n",
    "    right_eye_pts = landmarks[42:48]\n",
    "\n",
    "    left_eye_center  = left_eye_pts.mean(axis=0)\n",
    "    right_eye_center = right_eye_pts.mean(axis=0)\n",
    "\n",
    "    # Compute angle between eye centers\n",
    "    dY = right_eye_center[1] - left_eye_center[1]\n",
    "    dX = right_eye_center[0] - left_eye_center[0]\n",
    "    angle = np.degrees(np.arctan2(dY, dX))\n",
    "\n",
    "    # Compute scale based on desired eye distance\n",
    "    dist = np.sqrt((dX ** 2) + (dY ** 2))\n",
    "    desired_right_eye_x = 1.0 - desired_left_eye[0]\n",
    "    desired_dist = (desired_right_eye_x - desired_left_eye[0]) * desired_size\n",
    "    scale = desired_dist / dist\n",
    "\n",
    "    # Eye center midpoint\n",
    "    eyes_center = ((left_eye_center[0] + right_eye_center[0]) / 2,\n",
    "                   (left_eye_center[1] + right_eye_center[1]) / 2)\n",
    "\n",
    "    # Get affine rotation+scale matrix\n",
    "    M = cv2.getRotationMatrix2D(eyes_center, angle, scale)\n",
    "\n",
    "    # Shift so that eyes land in the desired position\n",
    "    tX = desired_size * 0.5\n",
    "    tY = desired_size * desired_left_eye[1]\n",
    "    M[0, 2] += (tX - eyes_center[0])\n",
    "    M[1, 2] += (tY - eyes_center[1])\n",
    "\n",
    "    # Apply affine transform\n",
    "    aligned = cv2.warpAffine(img_bgr, M, (desired_size, desired_size),\n",
    "                             flags=cv2.INTER_CUBIC)\n",
    "\n",
    "    return aligned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbf77890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Found 1857 input face crops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning train: 100%|██████████| 1857/1857 [00:20<00:00, 89.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Aligned: 1035, Skipped: 822. Saved to: /content/Attendance-System-Using-Face--2/aligned_faces_train\n",
      "[valid] Found 183 input face crops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning valid: 100%|██████████| 183/183 [00:01<00:00, 109.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] Aligned: 92, Skipped: 91. Saved to: /content/Attendance-System-Using-Face--2/aligned_faces_valid\n",
      "[test] Found 97 input face crops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning test: 100%|██████████| 97/97 [00:00<00:00, 120.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] Aligned: 47, Skipped: 50. Saved to: /content/Attendance-System-Using-Face--2/aligned_faces_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def align_split(split_name: str):\n",
    "    in_dir = ROOT / f\"cropped_faces_{split_name}\"\n",
    "    if not in_dir.exists():\n",
    "        print(f\"[{split_name}] Input dir not found, skipping:\", in_dir)\n",
    "        return\n",
    "\n",
    "    out_dir = ROOT / f\"aligned_faces_{split_name}\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    files = [f for f in os.listdir(in_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    print(f\"[{split_name}] Found {len(files)} input face crops\")\n",
    "\n",
    "    aligned_count = 0\n",
    "    skipped = 0\n",
    "    for fname in tqdm(files, desc=f\"Aligning {split_name}\"):\n",
    "        in_path = in_dir / fname\n",
    "        img = cv2.imread(str(in_path))\n",
    "        if img is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        aligned = align_face_dlib(img)\n",
    "        if aligned is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        out_path = out_dir / fname\n",
    "        cv2.imwrite(str(out_path), aligned)\n",
    "        aligned_count += 1\n",
    "\n",
    "    print(f\"[{split_name}] Aligned: {aligned_count}, Skipped: {skipped}. Saved to: {out_dir}\")\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    align_split(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2de3d988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train => 1035 aligned images\n",
      "valid => 92 aligned images\n",
      "test => 47 aligned images\n"
     ]
    }
   ],
   "source": [
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    out_dir = ROOT / f\"aligned_faces_{split}\"\n",
    "    if out_dir.exists():\n",
    "        print(split, \"=>\", len(os.listdir(out_dir)), \"aligned images\")\n",
    "    else:\n",
    "        print(split, \"=> no aligned dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16828d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train aligned: /content/Attendance-System-Using-Face--2/aligned_faces_train exists: True\n",
      "Valid aligned: /content/Attendance-System-Using-Face--2/aligned_faces_valid exists: True\n",
      "Test aligned: /content/Attendance-System-Using-Face--2/aligned_faces_test exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/content/Attendance-System-Using-Face--2\")\n",
    "\n",
    "aligned_train = ROOT / \"aligned_faces_train\"\n",
    "aligned_valid = ROOT / \"aligned_faces_valid\"\n",
    "aligned_test  = ROOT / \"aligned_faces_test\"\n",
    "\n",
    "print(\"Train aligned:\", aligned_train, \"exists:\", aligned_train.exists())\n",
    "print(\"Valid aligned:\", aligned_valid, \"exists:\", aligned_valid.exists())\n",
    "print(\"Test aligned:\",  aligned_test,  \"exists:\", aligned_test.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb10b45",
   "metadata": {},
   "source": [
    "### Augment Aligned Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "164fa04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Found 1035 aligned images in /content/Attendance-System-Using-Face--2/aligned_faces_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting aligned train: 100%|██████████| 1035/1035 [00:13<00:00, 77.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Augmentation complete. Check folder: /content/Attendance-System-Using-Face--2/aug_aligned_faces_train\n",
      "[valid] Found 92 aligned images in /content/Attendance-System-Using-Face--2/aligned_faces_valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting aligned valid: 100%|██████████| 92/92 [00:01<00:00, 82.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] Augmentation complete. Check folder: /content/Attendance-System-Using-Face--2/aug_aligned_faces_valid\n",
      "[test] Found 47 aligned images in /content/Attendance-System-Using-Face--2/aligned_faces_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting aligned test: 100%|██████████| 47/47 [00:00<00:00, 80.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] Augmentation complete. Check folder: /content/Attendance-System-Using-Face--2/aug_aligned_faces_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "ROOT = Path(\"/content/Attendance-System-Using-Face--2\")\n",
    "\n",
    "def augment_aligned_split(split_name: str):\n",
    "    in_dir = ROOT / f\"aligned_faces_{split_name}\"\n",
    "    if not in_dir.exists():\n",
    "        print(f\"[{split_name}] Input dir not found, skipping:\", in_dir)\n",
    "        return\n",
    "\n",
    "    out_dir = ROOT / f\"aug_aligned_faces_{split_name}\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    images = [f for f in os.listdir(in_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    print(f\"[{split_name}] Found {len(images)} aligned images in {in_dir}\")\n",
    "\n",
    "    for fname in tqdm(images, desc=f\"Augmenting aligned {split_name}\"):\n",
    "        in_path = in_dir / fname\n",
    "        img = cv2.imread(str(in_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        base_name = Path(fname).stem\n",
    "        ext = Path(fname).suffix\n",
    "\n",
    "        # 1) Save original aligned copy\n",
    "        cv2.imwrite(str(out_dir / f\"{base_name}_orig{ext}\"), img)\n",
    "\n",
    "        # 2) Blurs with different kernel sizes\n",
    "        for k in [3, 5, 7]:\n",
    "            blurred = add_gaussian_blur(img, ksize=k)\n",
    "            cv2.imwrite(str(out_dir / f\"{base_name}_blur{k}{ext}\"), blurred)\n",
    "\n",
    "        # 3) Gaussian noise with different variances\n",
    "        for v in [10, 20, 30]:\n",
    "            noisy = add_gaussian_noise(img, var=v)\n",
    "            cv2.imwrite(str(out_dir / f\"{base_name}_gn{v}{ext}\"), noisy)\n",
    "\n",
    "        # 4) Salt & pepper noise\n",
    "        for amt in [0.01, 0.02]:\n",
    "            sp = add_salt_pepper_noise(img, amount=amt)\n",
    "            cv2.imwrite(str(out_dir / f\"{base_name}_sp{int(amt*100)}{ext}\"), sp)\n",
    "\n",
    "    print(f\"[{split_name}] Augmentation complete. Check folder: {out_dir}\")\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    augment_aligned_split(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5df9c514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train => 9315 augmented aligned images\n",
      "valid => 828 augmented aligned images\n",
      "test => 423 augmented aligned images\n"
     ]
    }
   ],
   "source": [
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    out_dir = ROOT / f\"aug_aligned_faces_{split}\"\n",
    "    if out_dir.exists():\n",
    "        print(split, \"=>\", len(os.listdir(out_dir)), \"augmented aligned images\")\n",
    "    else:\n",
    "        print(split, \"=> no aug_aligned dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72a8eca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.17.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (1.26.4)\n",
      "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (10.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (2.32.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.11.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install facenet-pytorch torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9ae9a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (1.26.4)\n",
      "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (10.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (2.32.4)\n",
      "Requirement already satisfied: torch<2.3.0,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (2.2.2)\n",
      "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (0.17.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.11.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.15.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea0a92",
   "metadata": {},
   "source": [
    "### Load FaceNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be93b74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "FaceNet-like model loaded \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from facenet_pytorch.models.inception_resnet_v1 import InceptionResnetV1\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = InceptionResnetV1(pretrained='vggface2', classify=False).eval().to(device)\n",
    "print(\"FaceNet-like model loaded \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a27fbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PILLOW in /usr/local/lib/python3.12/dist-packages (10.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PILLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd1738d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found torch 2.2.2+cu121 torchvision 0.17.2+cu121\n"
     ]
    }
   ],
   "source": [
    "# Ensure a compatible PyTorch + torchvision is installed before importing `transforms`\n",
    "# This attempts to install CPU wheels from the official PyTorch index if imports fail.\n",
    "import sys, subprocess, importlib\n",
    "def pip_install(args):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\"] + args)\n",
    "try:\n",
    "    import torch, torchvision\n",
    "    print(\"Found torch\", torch.__version__, \"torchvision\", torchvision.__version__)\n",
    "except Exception as e:\n",
    "    print(\"torch/torchvision import failed (or missing). Installing compatible builds (CPU wheels). This may take a few minutes...\")\n",
    "    pip_install([\"install\", \"--upgrade\", \"pip\"])\n",
    "    # Use the official PyTorch CPU wheels index which is widely compatible.\n",
    "    try:\n",
    "        pip_install([\"install\", \"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"])\n",
    "    except Exception:\n",
    "        # Fall back to a simpler install if the index approach fails\n",
    "        pip_install([\"install\", \"torch\", \"torchvision\"])\n",
    "    importlib.invalidate_caches()\n",
    "    torch = importlib.import_module(\"torch\")\n",
    "    torchvision = importlib.import_module(\"torchvision\")\n",
    "    print(\"Installed torch\", torch.__version__, \"torchvision\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1422c8e",
   "metadata": {},
   "source": [
    "### Building Dataset for FaceNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca7f422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 9315\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/content/Attendance-System-Using-Face--2\")\n",
    "train_aug_dir = ROOT / \"aug_aligned_faces_train\"\n",
    "\n",
    "class FaceImageFolder(Dataset):\n",
    "    def __init__(self, folder: Path):\n",
    "        self.folder = folder\n",
    "        self.paths = sorted(\n",
    "            [\n",
    "                folder / f\n",
    "                for f in os.listdir(folder)\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "            ]\n",
    "        )\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((160, 160)),\n",
    "            transforms.ToTensor(),                      # [0,1]\n",
    "            transforms.Normalize([0.5, 0.5, 0.5],       # [-1,1]\n",
    "                                 [0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, str(path)\n",
    "\n",
    "train_dataset = FaceImageFolder(train_aug_dir)\n",
    "print(\"Train images:\", len(train_dataset))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524dd411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid images: 828\n"
     ]
    }
   ],
   "source": [
    "# Build validation dataset and dataloader (mirrors train)\n",
    "valid_aug_dir = ROOT / \"aug_aligned_faces_valid\"\n",
    "if not valid_aug_dir.exists():\n",
    "    print(\"Validation augmented folder not found:\", valid_aug_dir)\n",
    "valid_dataset = FaceImageFolder(valid_aug_dir)\n",
    "print(\"Valid images:\", len(valid_dataset))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fedc210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test images: 423\n"
     ]
    }
   ],
   "source": [
    "# Build test dataset and dataloader\n",
    "test_aug_dir = ROOT / \"aug_aligned_faces_test\"\n",
    "if not test_aug_dir.exists():\n",
    "    print(\"Test augmented folder not found:\", test_aug_dir)\n",
    "test_dataset = FaceImageFolder(test_aug_dir)\n",
    "print(\"Test images:\", len(test_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e78134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "ROOT = Path(\"/content/Attendance-System-Using-Face--2\")\n",
    "\n",
    "def preprocess_img(img: Image.Image):\n",
    "    img = img.resize((160, 160))\n",
    "    img_np = torch.from_numpy((np.array(img)).astype(\"float32\"))  # [H, W, C]\n",
    "    img_np = img_np.permute(2, 0, 1) / 255.0                      # [C, H, W]\n",
    "    img_np = (img_np - 0.5) / 0.5                                 # [-1, 1]\n",
    "    return img_np\n",
    "\n",
    "class FaceImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder: Path):\n",
    "        self.folder = folder\n",
    "        self.paths = sorted(\n",
    "            [\n",
    "                folder / f\n",
    "                for f in os.listdir(folder)\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img_t = preprocess_img(img)\n",
    "        return img_t, str(path)\n",
    "\n",
    "\n",
    "def compute_embeddings_for_folder(folder: Path, out_name: str, model, device=\"cuda\"):\n",
    "    dataset = FaceImageFolder(folder)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "    all_embs = []\n",
    "    all_paths = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, batch_paths in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            embs = model(imgs)              # (B, 512)\n",
    "            all_embs.append(embs.cpu().numpy())\n",
    "            all_paths.extend(batch_paths)\n",
    "\n",
    "    all_embs = np.concatenate(all_embs, axis=0)\n",
    "    all_paths = np.array(all_paths)\n",
    "\n",
    "    out_path = ROOT / out_name\n",
    "    np.savez(out_path, paths=all_paths, embeddings=all_embs)\n",
    "    print(f\"Saved {all_embs.shape[0]} embeddings to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc673b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch.models.inception_resnet_v1 import InceptionResnetV1\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = InceptionResnetV1(pretrained='vggface2', classify=False).eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4898c37",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3077590159.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_embeddings_for_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"aug_aligned_faces_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_facenet_embeddings.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcompute_embeddings_for_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"aug_aligned_faces_valid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valid_facenet_embeddings.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcompute_embeddings_for_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"aug_aligned_faces_test\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"test_facenet_embeddings.npz\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3618287118.py\u001b[0m in \u001b[0;36mcompute_embeddings_for_folder\u001b[0;34m(folder, out_name, model, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_paths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0membs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# (B, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mall_embs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mall_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/facenet_pytorch/models/inception_resnet_v1.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_6a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_7a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/facenet_pytorch/models/inception_resnet_v1.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/facenet_pytorch/models/inception_resnet_v1.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compute_embeddings_for_folder(ROOT / \"aug_aligned_faces_train\", \"train_facenet_embeddings.npz\", model, device)\n",
    "compute_embeddings_for_folder(ROOT / \"aug_aligned_faces_valid\", \"valid_facenet_embeddings.npz\", model, device)\n",
    "compute_embeddings_for_folder(ROOT / \"aug_aligned_faces_test\",  \"test_facenet_embeddings.npz\",  model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c9b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
